---
title: 雪球爬虫设计
layout: json-post
---

主要说明下整个爬虫代码的逻辑与处理流程。

### 功能分析

### 代码结构

按功能模块划分，

* vendors-外部库
* modules-主要模块
* tools-工具函数
* main.py-主程序入口

### 主要逻辑

> 初步假设每个循环功能模块在独立的线程中。

分为以下几个部分，

* 交互console
* 轮询是否有新消息
* 请求timeline
* 解析html数据
* 保存爬取的数据
* 获取代理
* 获取账号
* 更新账号关注
* 远程指令，参数获取
* 日志输出

#### 1.交互console

由`DEAMON`参数决定是否启动，如果不启动console程序将在后台运行。
主要完成命令的输入和运行状态的输出。

#### 2.轮询是否有新消息

通过下面的接口GET请求，
```
http://xueqiu.com/remind/unread.json?_=1458614506219
```

返回的数据如下，
```
{"status":0,"followers":0,"dm":0,"comments":0,"mentions":0,"notices":0,"events":0,"contact_friends":0,"weibo_friends":0,"events_id":[],"ptl_status":0,"ptl_last_modified":0}
```

账号需要重新登录会返回如下数据，
```
{"error_description":"遇到错误，请刷新页面或者重新登录帐号后再试","error_uri":"/remind/unread.json","error_code":"400016"}
```

所以可以在这里检测账号的状态。

其中`status`段就是未阅读的消息数目。

参数`frequency`接受1~1000的整数，是轮询的频率，单位是秒；理论上这个数值越高，账号越安全，不宜过大，防止一次爬取不及时和单次数据太多。
在雪球自己的web端这个参数大概是小于10s。

如果该部分的结果返回大于0，即有新的动态需要抓取。

> 抓取步骤应该在这一步来完成，因为timeline请求是得到最新的消息，这两步应该是阻塞运行。

#### 3.请求timeline

如果frequency合适，一次请求能够完成需求。
请求成功，把数据丢给解析模块；请求失败，抛出错误信息。

#### 4.解析html数据

读取队列的数据，进行解析操作，把解析完成的数据丢给存储模块。

> 注意，一定要在程序关闭前，把queue中的数据保存到磁盘上，防止程序意外关闭时queue的数据丢失。
程序启动时从本地备份区，读取恢复。

可以把解析规则，用参数传入。

#### 5.保存爬取的数据

读取队列数据，

> 注意，一定要在程序关闭前，把queue中的数据保存到磁盘上，防止程序意外关闭时queue的数据丢失。
程序启动时从本地备份区，读取恢复。

#### 6.获取代理

先拿到账号，再拿回一个可用代理，如果无代理返回，抛出无代理错误。
传入的参数为`lastip`，返回`lastip`；如果，而且`allowlike`为
`0`，则无论这个代理是否在使用都返回该ip；否则抛出。

判断逻辑，
如果`allowmax`参数为`num`，从代理库选出所有可用且已经代理了`num-1`个账号的代理，如果`lastip`在其中，则返回`lastip`；如果`lastip`不在其中且`allowlike`为`0`，抛出代理可用错误；如果`lastip`不在其中且`allowlike`为`1`，返回相近的ip，未找到则报错；
如果`allowmax`参数为`1`，从代理库选出所有可用且未被占用代理，如果lastip在其中，而且返回`lastip`；如果`lastip`不在其中且`allowlike`为`0`，抛出代理可用错误；如果`lastip`不在其中且`allowlike`为`1`，返回相近的ip，未找到则报错。

#### 7.获取账号

拿取账号，检测账号可用性；根据账号的`lastip`字段来获取代理ip，

#### 8.更新账号关注

从关注表中返回未被关注的账号，以及已经失效的uid对应的账号。
生成关注请求。

#### 9.远程指令，参数获取

WebSocket监视，接受指令回传。

#### 10.日志输出

接受参数`type`，默认为Logger引擎写入日志数据，默认的Logger为[console, 配置中的日志文件对象]。
